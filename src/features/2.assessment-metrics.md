# Assessment Metrics

Assessment Metrics is used to allow admin configuring custom metrics for an assessment to see the answers in a proper way.

## New DB Table

### `analytics_metrics`

* `id` id of this metrics
* `name` the name of this metrics
* `description` the description of this metrics
* `is_public` whether this is a public metrics in the metrics library
* `data_source` : which data source we are linking to for the calculation
  * For MVP, we only support `question`
* `data_type` defines how to link to an assessment question. It has the following valid value (MVP):
  * `scalar` : can only be linked to a radio button choice question of an assessment
* `aggregation` defines how to calculate the result. It has the following valid value: (when the `data_type` is `scalar`)
  * `average` : the result will be the average `weight` of the answers for this question
  * `sum` : the result will be the sum of the `weight` of the answers for this question
  * `count` : the result will be the count of the user that has answered that question
* `filter_type` : the type of filter. Valid value `role` (MVP)
* `filter_value` : the filter value.
  * When `filter_type` is `role`, the valid values are `participant, mentor, coordinator, admin`
* `default_calculation_frequency`

### `analytics_metrics_institutions`

Relationship between a metrics and an institution

* `id`
* `metrics_id`
* `institution_id`
* `requirement` has the following valid value:
  * `required` : without linking this metrics to an assessment, an experience can not go live
  * `recommanded` : an experience can go live without linking this metrics to an assessment, but there will be a warning
  * `not required` : an experience can go live without linking this metrics to an assessment
* `status`
  * `draft` : won't show on the experience page
  * `active` : show on the experience page
  * `archived` : won't show on the experience page

### `analytics_metrics_links`

Relationship between a metrics and a data source link in an experience

* `id`
* `metrics_id`
* `data_source_id` : the related data source id
  * When `data_source` = `question`, this id is the question id
* `experience_id`
* `institution_id`
* `calculation_frequency` : when to calculate the metrics
  * `on demand` : we only support on demand calculation for MVP

### `analytics_metrics_records`

The table to store the metrics report records

* `id`
* `metrics_question_id`
* `value` : metrics data value result
* `count` : how many data have we gone through to do the calculation
* `created` : date when the record gets calculated

## Workflow

### Create metrics

There will be a page on the institution level to see all the required/recommanded metrics and institution admin is able to create new metrics or use a metrics from the metrics library.

### Use metrics

On the experience dashboard, there will be a "metrics" tab. Admin can configure the metrics (i.e. link metrics to a specific assessment question).

`required` metrics and `recommanded` metrics will have special indicator to remind the admin to do the configuration. If they are not configured properly, when admin try to make the experience live, it will popup some warning message.

### Trigger metrics calculation

On MVP, we will only implement on demand calculation. There will be a button on the experience dashboard "metrics" tab to trigger the calculation

### Export metrics result report

There will be a button in the experience dashboard "metrics" tab to export the report CSV

Example report CSV:

| Metric         | Description    | Agg Method | 01/01/2024 Value | 01/01/2024 Count | 01/10/2024 Value | 01/10/2024 Count |
|----------------|-------------------|------------|------------|-----------------|-----------------|-----------------|
| WTR        | Willingness to Recommend      | Average    | 0.7             | 60        | 0.6        | 100             |
| Skill: Communication     | 1-5 scale self assessment of communication skill level, 1 being lowest, 5 highest | Average    | 0.9             | 120             | 0.8             | 150             |
| Opt-in for Research      | Count of students who answer Yes to allowing their data to be used for research     | Sum        | 140             | 160             | 170             | 200             |
| Provided Optional Feedback | Count of students who provided any answer at all to an optional text feedback question.  | Count      | 60              | 120             | 75              | 150             |

## Tasks

1. Create new db tables **[API]**
2. List all the metrics in the institution *(institution setting page)*
   1. Page for the list **[UI]**
   2. API for the list **[API]**
3. Add a new metrics *(institution setting page)*
   1. Add a fresh new metrics from a form **[UI]**
   2. API to create a new metrics with form data **[API]**
   3. Add a new metrics from the metrics library
      1. List the metrics in the library **[UI]**
      2. API to list the metrics in the library **[API]**
      3. API to add a new metrics with a given metrics id from library **[API]**
4. Edit metrics *(institution setting page)*
   1. Page to edit metrics **[UI]**
   2. API to edit metrics **[API]**
5. List all the metrics in the institution *(experience setting page)*
   1. Page for the list (indicate whether each metrics is configured or not) **[UI]**
   2. API for the list (including the configuration in the experience) **[API]**
6. Configure a metrics in the experience. i.e. link a metrics to an assessment question *(experience setting page)*
   1. Page to link metrics to a question (inside an experience) **[UI]**
   2. API to list assessment questions **[API]**
   3. API to link metrics to a question **[API]**
7. Trigger the calculation of a single metrics or all metrics *(experience setting page)*
   1. A button to trigger the calculation **[UI]**
   2. API to do the calculation **[API]**
8. Download the report CSV *(experience setting page)*
   1. A button to download the report CSV **[UI]**
   2. API to respond with report data **[API]**
